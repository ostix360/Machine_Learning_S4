{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c47abf6",
   "metadata": {},
   "source": [
    "# Réseau de neurones from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaf59a",
   "metadata": {},
   "source": [
    "## Définition des classes principales\n",
    "\n",
    "Ce notebook implémente un réseau de neurones simple en utilisant NumPy. Les composants clés sont :\n",
    "\n",
    "- **Parameter** : Conteneur pour les poids et biais apprenables du réseau.\n",
    "- **Function** : Classe de base pour les opérations (addition, multiplication, activations, perte) et leur différentiation (méthodes `forward` et `backward`).\n",
    "- **Module** : Classe de base pour les couches et le réseau, gérant les paramètres et les sous-modules.\n",
    "- **Linear** : Couche linéaire (transformation affine).\n",
    "- **Layer** : Combine une couche linéaire avec une fonction d'activation.\n",
    "- **Adam** : Algorithme d'optimisation pour mettre à jour les paramètres.\n",
    "- **NeuralNetwork** : Le réseau de neurones complet, assemblant les couches.\n",
    "- **Dataset** : Classe utilitaire pour générer des données synthétiques.\n",
    "- **LR_Scheduler** : Classe pour ajuster le taux d'apprentissage pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9601a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf3043",
   "metadata": {},
   "source": [
    "### Classe `Parameter`\n",
    "\n",
    "Encapsule les tenseurs (ici, des tableaux NumPy) qui sont les paramètres apprenables du modèle (poids et biais). Elle stocke la valeur du paramètre (`data`) et son gradient (`_grad`), ainsi qu'un indicateur (`_requires_grad`) pour savoir si le gradient doit être calculé pour ce paramètre, ici ce sera toujours le cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab40253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"Stocke les paramètres apprenables et leur gradient.\"\"\"\n",
    "    def __init__(self, data, requires_grad=True):\n",
    "        self._requires_grad = requires_grad # Indique si le gradient doit être calculé\n",
    "        self._grad = None # Stocke le gradient calculé lors de la rétropropagation\n",
    "        self.data = data # La valeur actuelle du paramètre (poids ou biais)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062e5f3",
   "metadata": {},
   "source": [
    "### Classe `Module`\n",
    "\n",
    "Classe de base pour toutes les couches et le réseau lui-même. Elle fournit des fonctionnalités pour enregistrer des sous-modules et des paramètres, facilitant la gestion de structures de réseau complexes. Elle permet également d'appeler directement une instance de module comme une fonction, ce qui exécute sa méthode `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256d75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"Classe de base pour les couches et le réseau.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modules = {} # Dictionnaire pour stocker les sous-modules (ex: couches dans un réseau)\n",
    "        self.parameters = [] # Liste pour stocker les paramètres de ce module et de ses sous-modules\n",
    "        self.fns = [] # Liste temporaire pour stocker les fonctions utilisées lors du forward pass (pour le backward)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Permet d'appeler le module comme une fonction, exécutant le forward pass.\"\"\"\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "    def add_module(self, name, module):\n",
    "        \"\"\"Ajoute un sous-module (ex: une couche) à ce module.\"\"\"\n",
    "        if not isinstance(module, Module):\n",
    "            raise TypeError(\"module must be an instance of Module\")\n",
    "        self.modules.update({name: module})\n",
    "        # Ajoute les paramètres du sous-module à la liste des paramètres du module parent\n",
    "        self.parameters.extend(module.get_parameters())\n",
    "\n",
    "    def clear_fns(self):\n",
    "        \"\"\"Vide la liste des fonctions utilisées lors du forward pass.\"\"\"\n",
    "        self.fns = []\n",
    "\n",
    "    def register_parameter(self, parameter):\n",
    "        \"\"\"Enregistre un paramètre directement dans ce module.\"\"\"\n",
    "        self.parameters.append(parameter)\n",
    "\n",
    "    def register_function(self, fn):\n",
    "        \"\"\"Enregistre une fonction utilisée pendant le forward pass (utile pour le backward).\"\"\"\n",
    "        self.fns.append(fn)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Retourne la liste de tous les paramètres (propres et des sous-modules).\"\"\"\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d694f",
   "metadata": {},
   "source": [
    "### Classe `Function`\n",
    "\n",
    "Classe de base pour toutes les opérations différentiables (couches, activations, fonctions de perte). Chaque fonction doit implémenter :\n",
    "- `forward`: Calcule la sortie de la fonction.\n",
    "- `backward`: Calcule le gradient de la perte par rapport à l'entrée de la fonction, en utilisant le gradient de la perte par rapport à la sortie (fourni en argument). C'est l'étape clé de la rétropropagation, basée sur la règle de dérivation en chaîne.\n",
    "\n",
    "Lors de l'appel (`__call__`), l'entrée et la sortie sont stockées pour être utilisées lors du `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8037ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    \"\"\"Classe de base pour les opérations différentiables.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._input = None # Stocke l'entrée(s) du forward pass\n",
    "        self._output = None # Stocke la sortie du forward pass\n",
    "        self._grad_input = None # Stocke le gradient calculé par rapport à l'entrée\n",
    "        self._grad_output = None # Stocke le gradient reçu de la couche suivante\n",
    "\n",
    "    def forward(self, *args):\n",
    "        \"\"\"Calcule la sortie de la fonction.\"\"\"\n",
    "        raise NotImplementedError(\"forward method not implemented\")\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        \"\"\"Calcule le gradient par rapport à l'entrée.\"\"\"\n",
    "        raise NotImplementedError(\"backward method not implemented\")\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        \"\"\"Exécute le forward pass et stocke l'entrée/sortie.\"\"\"\n",
    "        self._input = args\n",
    "        self._output = self.forward(*args)\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c27d1c",
   "metadata": {},
   "source": [
    "### Théorie Mathématique : Forward et Backward Propagation\n",
    "\n",
    "#### Notations\n",
    "Considérons un réseau de neurones feedforward avec $L$ couches.\n",
    "- $n$: Indice de couche ($n=1, ..., L$). La couche 0 est l'entrée $x$.\n",
    "- $y^{(n)}$: Vecteur d'activation (sortie) de la couche $n$. $y^{(0)} = x$.\n",
    "- $z^{(n)}$: Vecteur de pré-activation de la couche $n$.\n",
    "- $W^{(n)}$: Matrice des poids de la couche $n$. $W_{ij}^{(n)}$ est le poids connectant le neurone $j$ de la couche $n-1$ au neurone $i$ de la couche $n$.\n",
    "- $b^{(n)}$: Vecteur des biais de la couche $n$.\n",
    "- $\\sigma^{(n)}$: Fonction d'activation de la couche $n$. $\\sigma$ sans indice représente une fonction d'activation générique.\n",
    "- $\\mathcal{L}$: Fonction de perte (Loss).\n",
    "- $N$: Taille du batch.\n",
    "\n",
    "#### Forward Propagation (Passe Avant)\n",
    "Le but est de calculer la prédiction $y^{(L)} = y_{pred}$ à partir de l'entrée $x = y^{(0)}$.\n",
    "Pour chaque couche $n$ de $1$ à $L$:\n",
    "1.  **Transformation Linéaire**: Calcul de la pré-activation.\n",
    "    $$ z^{(n)} = W^{(n)} y^{(n-1)} + b^{(n)} $$\n",
    "    En termes d'indices : $ z_i^{(n)} = \\sum_j W_{ij}^{(n)} y_j^{(n-1)} + b_i^{(n)} $\n",
    "    *   **Code**: Ceci est implémenté dans `Linear.forward`. L'opération $W^{(n)} y^{(n-1)}$ correspond à `Mult()(y_prev, self.weight.data)` et l'addition $+ b^{(n)}$ correspond à `Add()(res_mult, self.bias.data)`. Les instances `Mult` et `Add` sont enregistrées (`register_function`).\n",
    "\n",
    "2.  **Fonction d'Activation**: Calcul de l'activation.\n",
    "    $$ y^{(n)} = \\sigma^{(n)}(z^{(n)}) $$\n",
    "    En termes d'indices : $ y_i^{(n)} = \\sigma^{(n)}(z_i^{(n)}) $\n",
    "    *   **Code**: Ceci est implémenté par l'appel à `self.activation(x)` dans `Layer.forward`, où `self.activation` est une instance d'une classe héritant de `Function` (e.g., `Tanh`, `ReLU`). La fonction d'activation enregistre également son entrée $z^{(n)}$ pour le backward pass.\n",
    "    *   Note: La dernière couche (`Linear` dans `NeuralNetwork`) n'a pas de fonction d'activation dans cette implémentation, donc $y^{(L)} = z^{(L)}$.\n",
    "\n",
    "3.  **Calcul de la Perte**: Après la dernière couche, on calcule la perte.\n",
    "    $$ \\mathcal{L} = L(y^{(L)}, y_{true}) $$\n",
    "    *   **Code**: Ceci est fait par `self.loss(x, y)` dans `NeuralNetwork.forward`, où `self.loss` est une instance de `MSELoss`. Elle enregistre $y^{(L)}$ et $y_{true}$.\n",
    "\n",
    "Le `NeuralNetwork.forward` orchestre ces étapes en appelant séquentiellement les `forward` des `Layer` (qui contiennent `Linear` et `Activation`) et enfin le `forward` de la `MSELoss`.\n",
    "\n",
    "#### Backward Propagation (Rétropropagation)\n",
    "Le but est de calculer les gradients de la perte $\\mathcal{L}$ par rapport à tous les paramètres ($W^{(n)}$, $b^{(n)}$ pour $n=1..L$) en utilisant la règle de dérivation en chaîne.\n",
    "\n",
    "On calcule les gradients couche par couche, en partant de la fin.\n",
    "**Notation de gradient**: $\\delta^{(n)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(n)}}$ (gradient de la perte par rapport à la pré-activation de la couche $n$).\n",
    "\n",
    "1.  **Initialisation (Couche de sortie L)**:\n",
    "    *   Calculer le gradient de la perte par rapport à la sortie du réseau:\n",
    "      $$ \\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}} $$\n",
    "      *   **Code**: C'est le point de départ, calculé par `self.loss.backward()`. Pour `MSELoss`, $\\frac{\\partial \\mathcal{L}}{\\partial y_i^{(L)}} = 2(y_i^{(L)} - y_{true, i})$. Le code `MSELoss.backward` retourne $2 * (y_{pred} - y_{true})$.\n",
    "    *   Calculer $\\delta^{(L)}$:\n",
    "      $$ \\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}} \\odot \\sigma^{(L)\\prime}(z^{(L)}) $$\n",
    "      où $\\odot$ est la multiplication élément par élément et $\\sigma^{(L)\\prime}$ est la dérivée de la fonction d'activation de la couche L.\n",
    "      *   **Code**: Si la dernière couche est `Linear` (pas d'activation $\\sigma^{(L)}$), alors $y^{(L)}=z^{(L)}$ et $\\sigma^{(L)\\prime}=1$, donc $\\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}}$. Le gradient $\\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}}$ (retourné par `loss.backward`) est directement passé comme `grad_output` à `self.layers[-1].backward()` (qui est un `Linear.backward`).\n",
    "      *   Si la dernière couche était un `Layer` avec activation, `Layer.backward` appellerait d'abord `activation.backward(grad_output)` qui calcule $\\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}} \\odot \\sigma^{(L)\\prime}(z^{(L)})$.\n",
    "\n",
    "2.  **Propagation vers l'arrière (Pour n de L à 1)**:\n",
    "    Supposons que nous ayons calculé $\\delta^{(n)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(n)}}$.\n",
    "    *   **Calcul des gradients des paramètres de la couche n**: \n",
    "      $$ \\frac{\\partial \\mathcal{L}}{\\partial W^{(n)}} = \\delta^{(n)} (y^{(n-1)})^T \\frac{1}{N} $$\n",
    "      $$ \\frac{\\partial \\mathcal{L}}{\\partial b^{(n)}} = \\sum_{batch} \\delta^{(n)} \\frac{1}{N}$$\n",
    "       (Somme ou moyenne sur les exemples du batch)\n",
    "      *   **Code**: Ceci est calculé dans `Linear.backward`. Le `grad_output` reçu par `Linear.backward` est $\\delta^{(n)}$.\n",
    "          *   `Add.backward(grad_output)` (où `grad_output` est $\\delta^{(n)}$) calcule $\\frac{\\partial \\mathcal{L}}{\\partial b^{(n)}} = \\sum_{batch} \\delta^{(n)} \\frac{1}{N}$ (variable `db`) et retourne aussi $\\delta^{(n)}$ (variable `dx_add`).\n",
    "          *   `Mult.backward(dx_add)` (où `dx_add` est $\\delta^{(n)}$) utilise l'entrée enregistrée $y^{(n-1)}$ pour calculer $\\frac{\\partial \\mathcal{L}}{\\partial W^{(n)}} = (y^{(n-1)})^T \\delta^{(n)} \\frac{1}{N}$ (variable `dw`, après transposition et moyenne sur le batch) et $\\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}} = \\delta^{(n)} (W^{(n)})^T$ (variable `dx`).\n",
    "          *   Les gradients `dw` et `db` sont stockés dans `self.weight._grad` et `self.bias._grad`.\n",
    "    *   **Calcul du gradient à propager à la couche n-1**: \n",
    "      $$ \\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}} = (W^{(n)})^T \\delta^{(n)} $$\n",
    "      *   **Code**: C'est le gradient `dx` retourné par `Mult.backward` et ensuite par `Linear.backward`.\n",
    "    *   **Calcul de $\\delta^{(n-1)}$ pour la couche précédente**: \n",
    "      $$ \\delta^{(n-1)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(n-1)}} = \\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}} \\odot \\sigma^{(n-1)\\prime}(z^{(n-1)}) $$\n",
    "      *   **Code**: Le gradient $\\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}}$ (retourné par `Linear.backward` de la couche $n$) est passé comme `grad_output` à `Layer.backward` de la couche $n-1$.\n",
    "      *   `Layer.backward` appelle d'abord `activation.backward(grad_output)` qui calcule $\\delta^{(n-1)} = \\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}} \\odot \\sigma^{(n-1)\\prime}(z^{(n-1)})$.\n",
    "      *   Ce $\\delta^{(n-1)}$ est ensuite passé comme `grad_output` à `self.linear.backward` pour calculer les gradients des paramètres $W^{(n-1)}, b^{(n-1)}$ et le gradient $\\frac{\\partial \\mathcal{L}}{\\partial y^{(n-2)}}$ à propager.\n",
    "\n",
    "Le `NeuralNetwork.backward` initie ce processus en appelant `loss.backward()` et propage ensuite le gradient en appelant les `backward` des couches en ordre inverse. Chaque `backward` de `Layer`, `Linear`, `Activation`, `Add`, `Mult` implémente une étape de la règle de chaîne, calculant soit les gradients des paramètres locaux, soit le gradient à propager à l'étape précédente, soit les deux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d7266",
   "metadata": {},
   "source": [
    "### Implémentations de `Function`\n",
    "\n",
    "Implémentations spécifiques pour différentes opérations : addition, multiplication matricielle, fonctions d'activation (ReLU, Identity, Arctan, Tanh) et fonction de perte (MSELoss).\n",
    "\n",
    "**Principe du `backward` (Rétropropagation)**\n",
    "\n",
    "La méthode `backward(grad_output)` reçoit `grad_output`, qui est le gradient de la perte finale par rapport à la *sortie* de cette fonction (`dLoss / dOutput`). Son objectif est de calculer le gradient de la perte par rapport à l'*entrée* de cette fonction (`dLoss / dInput`).\n",
    "\n",
    "Selon la règle de dérivation en chaîne :\n",
    "`dLoss / dInput = (dLoss / dOutput) * (dOutput / dInput)`\n",
    "\n",
    "Donc, chaque méthode `backward` doit :\n",
    "1. Calculer la dérivée locale de la sortie par rapport à l'entrée (`dOutput / dInput`), en utilisant potentiellement les valeurs d'entrée stockées (`self._input`).\n",
    "2. Multiplier cette dérivée locale par le gradient reçu (`grad_output`).\n",
    "3. Retourner le résultat (`dLoss / dInput`), qui sera passé comme `grad_output` à la fonction précédente dans le calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1b2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    \"\"\"Addition élément par élément.\"\"\"\n",
    "    def forward(self, x, y):\n",
    "        # y est souvent un biais, x les activations. Leurs dernières dimensions doivent correspondre.\n",
    "        assert x.shape[-1] == y.shape[-1], f\"x and y must have the same shape but got {x.shape[-1]} and {y.shape[-1]}\"\n",
    "        return x + y\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx et dLoss/dy. \"\"\"\n",
    "        # dOutput/dx = 1, dOutput/dy = 1 (pour chaque élément)\n",
    "        # dLoss/dx = dLoss/dOutput * dOutput/dx = grad_output * 1\n",
    "        # dLoss/dy = dLoss/dOutput * dOutput/dy = grad_output * 1\n",
    "        # Pour le biais (y), on somme les gradients sur la dimension du batch (axis=0)\n",
    "        # car le même biais est ajouté à chaque exemple du batch.\n",
    "        # On divise par la taille du batch pour obtenir une moyenne.\n",
    "        return grad_output, grad_output.sum(axis=0) / grad_output.shape[0]\n",
    "    \n",
    "class Mult(Function):\n",
    "    \"\"\"Multiplication matricielle.\"\"\"\n",
    "    def forward(self, x, y):\n",
    "        # x: (batch_size, in_features), y: (in_features, out_features)\n",
    "        assert x.shape[-1] == y.shape[-2], f\"impossible to compute matmult due to wrong shape (got {x.shape} and {y.shape})\"\n",
    "        self._input = (x, y) # Stocker x et y pour le backward\n",
    "        output = x @ y\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx et dLoss/dy (où y sont les poids). \"\"\"\n",
    "        # grad_output: (batch_size, out_features)\n",
    "        x, y = self._input # x: (batch_size, in_features), y: (in_features, out_features)\n",
    "        \n",
    "        # Règle de dérivation matricielle:\n",
    "        # d(X@Y)/dX = grad_output @ Y.T\n",
    "        # d(X@Y)/dY = X.T @ grad_output\n",
    "        \n",
    "        # dLoss/dy = (dLoss/dOutput) * (dOutput/dy) = X.T @ grad_output\n",
    "        # On moyenne sur le batch pour le gradient des poids (y)\n",
    "        grad_y = x.T @ grad_output / x.shape[0]\n",
    "        \n",
    "        # dLoss/dx = (dLoss/dOutput) * (dOutput/dx) = grad_output @ Y.T\n",
    "        grad_x = grad_output @ y.T\n",
    "        \n",
    "        return grad_x, grad_y\n",
    "    \n",
    "class ReLU(Function):\n",
    "    \"\"\"Fonction d'activation ReLU (Rectified Linear Unit).\"\"\"\n",
    "    def __init__(self, eps=0): # eps permet d'implémenter Leaky ReLU\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._input = x # Stocker x pour le backward\n",
    "        return np.maximum(self.eps * x, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx. \"\"\"\n",
    "        x = self._input\n",
    "        # dOutput/dx = 1 si x > 0, sinon eps\n",
    "        # dLoss/dx = dLoss/dOutput * dOutput/dx\n",
    "        grad_input = np.where(x > 0, grad_output, self.eps * grad_output)\n",
    "        return grad_input\n",
    "    \n",
    "class Identity(Function):\n",
    "    \"\"\"Fonction identité (pas d'activation).\"\"\"\n",
    "    def forward(self, x):\n",
    "        self._input = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx. \"\"\"\n",
    "        # dOutput/dx = 1\n",
    "        # dLoss/dx = dLoss/dOutput * 1\n",
    "        return grad_output    \n",
    "\n",
    "class Arctan(Function):\n",
    "    \"\"\"Fonction d'activation Arctangente.\"\"\"\n",
    "    def forward(self, x):\n",
    "        self._input = x\n",
    "        return np.arctan(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx. \"\"\"\n",
    "        x = self._input\n",
    "        # d(arctan(x))/dx = 1 / (1 + x**2)\n",
    "        # dLoss/dx = dLoss/dOutput * (1 / (1 + x**2))\n",
    "        grad_input = grad_output / (1 + x**2)\n",
    "        return grad_input\n",
    "    \n",
    "class Tanh(Function):\n",
    "    \"\"\"Fonction d'activation Tangente Hyperbolique.\"\"\"\n",
    "    def forward(self, x):\n",
    "        self._input = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Calcule dLoss/dx. \"\"\"\n",
    "        x = self._input\n",
    "        # d(tanh(x))/dx = 1 - tanh(x)**2\n",
    "        # dLoss/dx = dLoss/dOutput * (1 - tanh(x)**2)\n",
    "        grad_input = grad_output * (1 - np.tanh(x)**2)\n",
    "        return grad_input\n",
    "    \n",
    "class MSELoss(Function):\n",
    "    \"\"\"Fonction de perte : Erreur Quadratique Moyenne (Mean Squared Error).\"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred: prédictions du modèle, y_true: vraies valeurs\n",
    "        self._input = (y_pred, y_true)\n",
    "        # Calcule la moyenne des carrés des différences\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, grad_output=1.0): # grad_output est souvent 1 pour la fonction de perte\n",
    "        \"\"\" Calcule dLoss/dy_pred. \"\"\"\n",
    "        y_pred, y_true = self._input\n",
    "        # Loss = (1/N) * sum((y_pred - y_true)**2)\n",
    "        # dLoss/dy_pred = (1/N) * sum(2 * (y_pred - y_true))\n",
    "        # Le calcul de la moyenne (1/N) est implicitement géré par la moyenne effectuée\n",
    "        # dans le backward de la multiplication matricielle lors du calcul du gradient des poids.\n",
    "        # Ici, on retourne le gradient avant la moyenne sur le batch.\n",
    "        grad_input = (2 * (y_pred - y_true))\n",
    "        # On multiplie par grad_output (qui est généralement 1.0 ici)\n",
    "        return grad_input * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f72222",
   "metadata": {},
   "source": [
    "### Classe `Linear`\n",
    "\n",
    "Implémente une couche linéaire (ou dense) : `output = input @ weight + bias`.\n",
    "\n",
    "- **Forward**: Effectue la multiplication matricielle et ajoute le biais (s'il existe). Enregistre les fonctions `Mult` et `Add` utilisées.\n",
    "- **Backward**: Reçoit le gradient de la couche suivante (`grad_output`). Utilise les fonctions `Add` et `Mult` enregistrées (dans l'ordre inverse) pour propager le gradient vers l'entrée (`dx`) et calculer les gradients des poids (`dw`) et du biais (`db`). Ces gradients sont stockés dans les objets `Parameter` correspondants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9ba4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"Couche linéaire: y = x @ W + b.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Initialisation des poids (distribution normale faible)\n",
    "        self.weight = Parameter(np.random.normal(0, 0.02, size=(in_features, out_features)))\n",
    "        self.register_parameter(self.weight)\n",
    "        if bias:\n",
    "            # Initialisation du biais\n",
    "            self.bias = Parameter(np.random.normal(0, 0.02, size=(out_features)))\n",
    "            self.register_parameter(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, in_features)\n",
    "        # weight: (in_features, out_features)\n",
    "        # bias: (out_features,)\n",
    "        # output: (batch_size, out_features)\n",
    "        if self.bias is not None:\n",
    "            # 1. Multiplication: x @ weight\n",
    "            m = Mult()\n",
    "            self.register_function(m) # Enregistrer pour le backward\n",
    "            res = m(x, self.weight.data)\n",
    "            # 2. Addition: res + bias\n",
    "            a = Add()\n",
    "            self.register_function(a) # Enregistrer pour le backward\n",
    "            res = a(res, self.bias.data)\n",
    "            return res\n",
    "        else:\n",
    "            # Seulement multiplication si pas de biais\n",
    "            m = Mult()\n",
    "            self.register_function(m)\n",
    "            res = m(x, self.weight.data)\n",
    "            return res\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Rétropropagation à travers la couche linéaire. \"\"\"\n",
    "        if self.bias is not None:\n",
    "            # 1. Backward de l'addition (a)\n",
    "            # Reçoit grad_output (dLoss/dOutput_Add)\n",
    "            # Retourne dLoss/dInput_Add (gradient avant l'addition) et dLoss/dBias\n",
    "            dx_add, db = self.fns[-1].backward(grad_output) # self.fns[-1] est 'a'\n",
    "            # 2. Backward de la multiplication (m)\n",
    "            # Reçoit dx_add (dLoss/dOutput_Mult)\n",
    "            # Retourne dLoss/dInput_Mult (gradient avant la multiplication) et dLoss/dWeight\n",
    "            dx, dw = self.fns[-2].backward(dx_add) # self.fns[-2] est 'm'\n",
    "            # Stocker les gradients calculés dans les paramètres\n",
    "            self.weight._grad = dw\n",
    "            self.bias._grad = db\n",
    "            self.clear_fns() # Nettoyer les fonctions enregistrées\n",
    "            return dx # Retourner le gradient par rapport à l'entrée de la couche linéaire\n",
    "        else:\n",
    "            # Seulement backward de la multiplication si pas de biais\n",
    "            dx, dw = self.fns[-1].backward(grad_output) # self.fns[-1] est 'm'\n",
    "            self.weight._grad = dw\n",
    "            self.clear_fns()\n",
    "            return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2414cd",
   "metadata": {},
   "source": [
    "### Classe `Layer`\n",
    "\n",
    "Combine une couche `Linear` suivie d'une fonction d'activation (`Function`).\n",
    "\n",
    "- **Forward**: Applique d'abord la couche linéaire, puis la fonction d'activation.\n",
    "- **Backward**: Applique d'abord le `backward` de la fonction d'activation au gradient reçu, puis passe le résultat au `backward` de la couche linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f673d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "    \"\"\"Combine une couche linéaire et une fonction d'activation.\"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(in_features, out_features)\n",
    "        # Utilise ReLU par défaut si aucune activation n'est fournie\n",
    "        self.activation = ReLU() if activation is None else activation\n",
    "        # Enregistre la couche linéaire comme sous-module\n",
    "        self.add_module('linear', self.linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Passe avant : linéaire puis activation. \"\"\"\n",
    "        x = self.linear.forward(x)\n",
    "        if self.activation is not None:\n",
    "            # Applique l'activation (qui enregistre aussi l'entrée/sortie)\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Passe arrière : activation puis linéaire. \"\"\"\n",
    "        if self.activation is not None:\n",
    "            # 1. Backward de l'activation\n",
    "            # Reçoit grad_output (dLoss/dOutput_Activation)\n",
    "            # Retourne dLoss/dInput_Activation\n",
    "            grad_output = self.activation.backward(grad_output)\n",
    "        # 2. Backward de la couche linéaire\n",
    "        # Reçoit le gradient après l'activation (dLoss/dOutput_Linear)\n",
    "        # Retourne dLoss/dInput_Linear\n",
    "        dx = self.linear.backward(grad_output)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5ae51",
   "metadata": {},
   "source": [
    "### Classe `Adam`\n",
    "\n",
    "Implémente l'optimiseur Adam (Adaptive Moment Estimation). C'est un algorithme d'optimisation stochastique populaire qui calcule des taux d'apprentissage adaptatifs pour chaque paramètre.\n",
    "\n",
    "Il maintient deux moyennes mobiles exponentielles :\n",
    "1. `m`: Moyenne des gradients passés (moment d'ordre 1).\n",
    "2. `v`: Moyenne des carrés des gradients passés (moment d'ordre 2).\n",
    "\n",
    "La méthode `step` met à jour les paramètres du modèle en utilisant ces moyennes mobiles, après correction de biais, pour adapter le taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f40422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Optimiseur Adam.\"\"\"\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.params = params # Liste des Parameter à optimiser\n",
    "        self.lr = lr # Taux d'apprentissage\n",
    "        self.beta1 = beta1 # Facteur d'oubli pour le moment d'ordre 1 (m)\n",
    "        self.beta2 = beta2 # Facteur d'oubli pour le moment d'ordre 2 (v)\n",
    "        self.eps = eps # Petit terme pour éviter la division par zéro\n",
    "        # Initialise les moments m et v avec des zéros de même forme que les paramètres\n",
    "        self.m = [np.zeros_like(p.data) for p in params]\n",
    "        self.v = [np.zeros_like(p.data) for p in params]\n",
    "        self.t = 0 # Compteur de pas (itérations)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Effectue une étape d'optimisation (mise à jour des paramètres).\"\"\"\n",
    "        self.t += 1\n",
    "        if len(self.params) == 0:\n",
    "            raise ValueError(\"No parameters to optimize\")\n",
    "        \n",
    "        for i, p in enumerate(self.params):\n",
    "            if p._requires_grad:\n",
    "                # Vérifie si un gradient a été calculé pour ce paramètre\n",
    "                if p._grad is None:\n",
    "                    # print(f\"Warning: Gradient for parameter {i} is None. Skipping update.\")\n",
    "                    continue # Passer au paramètre suivant si pas de gradient\n",
    "                if (p._grad == 0).all():\n",
    "                    # print(f\"Warning: Gradient for parameter {i} is zero. Skipping update.\")\n",
    "                    continue # Passer si le gradient est nul\n",
    "                    \n",
    "                grad = p._grad # Gradient calculé lors du backward pass\n",
    "                \n",
    "                # Mise à jour des moyennes mobiles exponentielles\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "                \n",
    "                # Correction du biais des moments (surtout important au début)\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                # Calcul de la mise à jour du paramètre\n",
    "                update = self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "                \n",
    "                # Vérification de forme (sécurité)\n",
    "                assert p.data.shape == update.shape, f\"Shape mismatch: {p.data.shape} vs {update.shape}\"\n",
    "                \n",
    "                # Appliquer la mise à jour (descente de gradient)\n",
    "                p.data = p.data - update\n",
    "                \n",
    "                # Réinitialiser le gradient pour le prochain cycle forward/backward\n",
    "                p._grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9866e",
   "metadata": {},
   "source": [
    "### Classe `NeuralNetwork`\n",
    "\n",
    "Assemble les couches (`Layer` et `Linear`) pour former le réseau complet.\n",
    "\n",
    "- **Initialisation**: Crée les couches (une couche d'entrée, plusieurs couches cachées, une couche de sortie) et les enregistre comme sous-modules. Initialise également la fonction de perte.\n",
    "- **Forward**: Fait passer l'entrée `x` séquentiellement à travers toutes les couches. Si `y` (la cible) est fournie, calcule et retourne également la perte.\n",
    "- **Backward**: Initialise la rétropropagation en appelant le `backward` de la fonction de perte. Ensuite, propage le gradient à travers les couches en ordre inverse, de la sortie vers l'entrée, en appelant le `backward` de chaque couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0860ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(Module):\n",
    "    \"\"\"Le réseau de neurones complet.\"\"\"\n",
    "    def __init__(self, nb_layers=4, in_features=1, out_features=1, hidden_features=16, activation=Arctan()):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.loss = MSELoss() # Utilise MSE comme fonction de perte\n",
    "        self.register_function(self.loss) # Enregistre la fonction de perte\n",
    "        \n",
    "        # Couche d'entrée (Layer = Linear + Activation)\n",
    "        self.layers.append(Layer(in_features, hidden_features, activation))\n",
    "        # Couches cachées (Layer = Linear + Activation)\n",
    "        for _ in range(nb_layers):\n",
    "            layer = Layer(hidden_features, hidden_features, activation)\n",
    "            self.layers.append(layer)\n",
    "        # Couche de sortie (Linear seulement, pas d'activation finale ici)\n",
    "        self.layers.append(Linear(hidden_features, out_features))\n",
    "        \n",
    "        # Enregistre chaque couche comme sous-module\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.add_module(f'layer_{i}', layer)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\" Passe avant à travers tout le réseau. \"\"\"\n",
    "        # Propagation à travers les couches\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        # Si les cibles y sont fournies, calculer la perte\n",
    "        if y is not None:\n",
    "            loss = self.loss(x, y) # x est maintenant la prédiction finale y_pred\n",
    "            return x, loss\n",
    "        return x, None # Retourne seulement la prédiction si pas de cible\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        # Assure que l'appel direct exécute le forward défini ici\n",
    "        return super().__call__(*args, **kwds)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\" Passe arrière à travers tout le réseau. \"\"\"\n",
    "        # 1. Démarrer la rétropropagation à partir de la fonction de perte\n",
    "        # Calcule dLoss/dy_pred\n",
    "        grad_output = self.loss.backward()\n",
    "        \n",
    "        # 2. Propager le gradient en arrière à travers les couches (ordre inverse)\n",
    "        for i, layer in enumerate(reversed(self.layers)):\n",
    "            # Passe le gradient reçu à la méthode backward de la couche\n",
    "            # La méthode backward de la couche calcule les gradients de ses paramètres\n",
    "            # et retourne le gradient par rapport à son entrée.\n",
    "            grad_output = layer.backward(grad_output)\n",
    "            \n",
    "        # Le gradient final retourné est dLoss/dx (gradient par rapport à l'entrée du réseau)\n",
    "        # mais les gradients importants (poids/biais) ont été stockés dans les Parameters\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd398b3",
   "metadata": {},
   "source": [
    "### Test simple (débugging)\n",
    "\n",
    "Vérification du fonctionnement des passes `forward` et `backward` ainsi que de l'optimiseur `Adam` sur un petit exemple avec des poids et des données définis manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e38cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[2. 3.]\n",
      " [3. 4.]\n",
      " [4. 5.]]\n",
      "Weight 0:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Bias 0:\n",
      "[1. 2.]\n",
      "Weight 1:\n",
      "[[1.]\n",
      " [2.]]\n",
      "Bias 1:\n",
      "[-1.]\n",
      "Output (Predictions):\n",
      "[[3.51825074]\n",
      " [3.56668501]\n",
      " [3.59578859]]\n",
      "Target:\n",
      "[[3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "Loss: 0.809385127304723\n",
      "Gradient w.r.t. input (dx):\n",
      "[[ 0.01990523  0.04695874]\n",
      " [-0.00937993 -0.02213197]\n",
      " [-0.01947157 -0.0459467 ]]\n",
      "Weight 0 grad:\n",
      "[[-0.00794464 -0.00706362]\n",
      " [-0.00902043 -0.00801677]]\n",
      "Bias 0 grad:\n",
      "[-0.00107579 -0.00095315]\n",
      "Weight 1 grad:\n",
      "[[-1.34546814]\n",
      " [-1.35749428]]\n",
      "Bias 1 grad:\n",
      "[-0.8795171]\n",
      "--- After Adam Step ---\n",
      "Updated Weight 0:\n",
      "[[1.00999999 2.00999999]\n",
      " [3.00999999 4.00999999]]\n",
      "Updated Bias 0:\n",
      "[1.00999991 2.0099999 ]\n",
      "Updated Weight 1:\n",
      "[[1.01]\n",
      " [2.01]]\n",
      "Updated Bias 1:\n",
      "[-0.99]\n"
     ]
    }
   ],
   "source": [
    "# Création d'un petit réseau (0 couche cachée => 1 Layer + 1 Linear)\n",
    "nn = NeuralNetwork(nb_layers=0, in_features=2, out_features=1, hidden_features=2)\n",
    "\n",
    "# Accès aux couches pour définir les poids manuellement\n",
    "layer0 = nn.layers[0] # Première couche (Layer = Linear + Activation)\n",
    "linear0 = layer0.linear # Couche linéaire dans layer0\n",
    "layer1 = nn.layers[1] # Deuxième couche (Linear de sortie)\n",
    "\n",
    "# Définition manuelle des poids et biais\n",
    "linear0.weight.data = np.array([[1., 2.], [3., 4.]]) # (in=2, hidden=2)\n",
    "linear0.bias.data = np.array([1., 2.]) # (hidden=2)\n",
    "layer1.weight.data = np.array([[1.], [2.]]) # (hidden=2, out=1)\n",
    "layer1.bias.data = np.array([-1.]) # (out=1)\n",
    "\n",
    "# Données d'entrée et cibles (batch de 3 exemples)\n",
    "x = np.array([[2., 3.], [3., 4.], [4., 5.]]) # (batch=3, in=2)\n",
    "y_hat = np.array([[3.], [4.], [5.]]) # (batch=3, out=1)\n",
    "\n",
    "# Initialisation de l'optimiseur avec les paramètres du réseau\n",
    "lin_adam = Adam(nn.get_parameters(), lr=0.01)\n",
    "\n",
    "# --- Forward Pass ---\n",
    "y, l = nn(x, y_hat) # Calcul des prédictions (y) et de la perte (l)\n",
    "print(f\"Input:\\n{x}\")\n",
    "print(f\"Weight 0:\\n{linear0.weight.data}\")\n",
    "print(f\"Bias 0:\\n{linear0.bias.data}\")\n",
    "print(f\"Weight 1:\\n{layer1.weight.data}\")\n",
    "print(f\"Bias 1:\\n{layer1.bias.data}\")\n",
    "print(f\"Output (Predictions):\\n{y}\")\n",
    "print(f\"Target:\\n{y_hat}\")\n",
    "print(f\"Loss: {l}\")\n",
    "\n",
    "# --- Backward Pass ---\n",
    "dx = nn.backward() # Calcule les gradients (stockés dans les Parameter)\n",
    "print(f\"Gradient w.r.t. input (dx):\\n{dx}\")\n",
    "print(f\"Weight 0 grad:\\n{linear0.weight._grad}\")\n",
    "print(f\"Bias 0 grad:\\n{linear0.bias._grad}\")\n",
    "print(f\"Weight 1 grad:\\n{layer1.weight._grad}\")\n",
    "print(f\"Bias 1 grad:\\n{layer1.bias._grad}\")\n",
    "\n",
    "# --- Optimizer Step ---\n",
    "lin_adam.step() # Met à jour les poids et biais en utilisant les gradients calculés\n",
    "print(\"--- After Adam Step ---\")\n",
    "print(f\"Updated Weight 0:\\n{linear0.weight.data}\")\n",
    "print(f\"Updated Bias 0:\\n{linear0.bias.data}\")\n",
    "print(f\"Updated Weight 1:\\n{layer1.weight.data}\")\n",
    "print(f\"Updated Bias 1:\\n{layer1.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c771633",
   "metadata": {},
   "source": [
    "### Boucle d'entraînement simple\n",
    "\n",
    "Exécute plusieurs étapes d'entraînement (forward, backward, optimizer step) sur le même petit jeu de données pour observer la convergence de la perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dc2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Loop --- \n",
      "Iteration 0, Loss: 0.7748949895871119\n",
      "Iteration 20, Loss: 0.6244611787545358\n",
      "Iteration 40, Loss: 0.6094297361069584\n",
      "Iteration 60, Loss: 0.6023206775222462\n",
      "Iteration 80, Loss: 0.5939425034062823\n",
      "--- Predictions After Training ---\n",
      "Final Output (Predictions):\n",
      "[[3.92836028]\n",
      " [4.0118833 ]\n",
      " [4.06141396]]\n",
      "Target:\n",
      "[[3.]\n",
      " [4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Loop --- \")\n",
    "for i in range(100): # Nombre réduit d'itérations pour l'exemple\n",
    "    # Forward pass: Calculer prédictions et perte\n",
    "    y, l = nn(x, y_hat)\n",
    "    # Backward pass: Calculer les gradients\n",
    "    dx = nn.backward()\n",
    "    if i % 20 == 0: # Afficher la perte toutes les 20 itérations\n",
    "        print(f\"Iteration {i}, Loss: {l}\")\n",
    "    # Mise à jour des paramètres\n",
    "    lin_adam.step()\n",
    "\n",
    "# Vérifier les prédictions après entraînement\n",
    "print(\"--- Predictions After Training ---\")\n",
    "final_y, _ = nn(x)\n",
    "print(f\"Final Output (Predictions):\\n{final_y}\")\n",
    "print(f\"Target:\\n{y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099e3fe",
   "metadata": {},
   "source": [
    "### Classe `Dataset`\n",
    "\n",
    "Génère un jeu de données synthétiques pour entraîner le réseau à approximer la fonction sinus. Les données sont divisées en ensembles d'entraînement et d'évaluation, et structurées en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f263d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Génère des données synthétiques (x, sin(x)) et les divise en train/eval.\"\"\"\n",
    "    def __init__(self, min_max_value, nb_data=1000, eval_ratio=0.1, batch_size=32):\n",
    "        self.min_max_value = min_max_value # Plage des valeurs de x\n",
    "        self.nb_data = nb_data # Nombre total de points de données (avant batching)\n",
    "        self.eval_ratio = eval_ratio # Proportion des données pour l'évaluation\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Calcule le nombre de batches\n",
    "        nb_batches = nb_data // batch_size\n",
    "        \n",
    "        # Génère x uniformément entre -min_max_value et +min_max_value\n",
    "        # Forme: (nb_batches, batch_size, 1) pour correspondre à l'entrée du réseau\n",
    "        x_ = np.random.uniform(-min_max_value, min_max_value, size=(nb_batches, batch_size, 1))\n",
    "        # Calcule y = sin(x)\n",
    "        y_ = np.sin(x_)\n",
    "        \n",
    "        # Divise en ensembles d'entraînement et d'évaluation (par batch)\n",
    "        self.nb_eval = int(nb_batches * eval_ratio)\n",
    "        self.nb_train = nb_batches - self.nb_eval\n",
    "        \n",
    "        self.x_train = x_[:self.nb_train, :, :]\n",
    "        self.y_train = y_[:self.nb_train, :, :]\n",
    "        self.x_eval = x_[self.nb_train:, :, :]\n",
    "        self.y_eval = y_[self.nb_train:, :, :]\n",
    "        print(f\"Dataset created: {self.nb_train} training batches, {self.nb_eval} evaluation batches.\")\n",
    "\n",
    "    def get_train_data(self):\n",
    "        \"\"\"Retourne les données d'entraînement (x, y).\"\"\"\n",
    "        return self.x_train, self.y_train\n",
    "    \n",
    "    def get_eval_data(self):\n",
    "        \"\"\"Retourne les données d'évaluation (x, y).\"\"\"\n",
    "        return self.x_eval, self.y_eval\n",
    "    \n",
    "    def eval_len(self):\n",
    "        \"\"\"Retourne le nombre de batches d'évaluation.\"\"\"\n",
    "        return self.nb_eval\n",
    "    \n",
    "    def train_len(self):\n",
    "        \"\"\"Retourne le nombre de batches d'entraînement.\"\"\"\n",
    "        return self.nb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a34e20",
   "metadata": {},
   "source": [
    "### Classe `LR_Scheduler`\n",
    "\n",
    "Implémente une stratégie simple de réduction du taux d'apprentissage (learning rate decay). Ici, une décroissance exponentielle est utilisée pendant un nombre défini d'étapes, après quoi le taux d'apprentissage reste fixe à une valeur minimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "047c9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Scheduler:\n",
    "    \"\"\"Planificateur de taux d'apprentissage (décroissance exponentielle).\"\"\"\n",
    "    def __init__(self, optimizer, start_value=0.1, end_value=0.01, nb_steps=1000):\n",
    "        self.optimizer = optimizer # L'optimiseur dont le lr doit être ajusté\n",
    "        self.start_value = start_value # Taux d'apprentissage initial\n",
    "        self.end_value = end_value # Taux d'apprentissage final (plancher)\n",
    "        self.nb_steps = nb_steps # Nombre d'étapes sur lesquelles la décroissance s'applique\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Met à jour le taux d'apprentissage de l'optimiseur.\"\"\"\n",
    "        if self.current_step < self.nb_steps:\n",
    "            # Calcul du lr avec décroissance exponentielle\n",
    "            # Le facteur -5 contrôle la vitesse de décroissance\n",
    "            lr = self.start_value * np.exp(-4 * self.current_step / self.nb_steps)\n",
    "            # Assure que le lr ne descend pas en dessous de end_value pendant la décroissance\n",
    "            self.optimizer.lr = max(lr, self.end_value)\n",
    "            self.current_step += 1\n",
    "        else:\n",
    "            # Une fois les nb_steps atteints, fixe le lr à end_value\n",
    "            self.optimizer.lr = self.end_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0e3c1",
   "metadata": {},
   "source": [
    "### Initialisation du Modèle et de l'Optimiseur\n",
    "\n",
    "Création de l'instance du réseau de neurones avec les hyperparamètres souhaités (nombre de couches, taille des couches, fonction d'activation) et initialisation de l'optimiseur Adam avec les paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeed3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres du modèle\n",
    "hidden_features = 128\n",
    "nb_hidden_layers = 2\n",
    "activation_fn = Tanh() # Utilisation de Tanh comme activation\n",
    "\n",
    "# Création du modèle\n",
    "model = NeuralNetwork(nb_layers=nb_hidden_layers, \n",
    "                      in_features=1, \n",
    "                      out_features=1, \n",
    "                      hidden_features=hidden_features, \n",
    "                      activation=activation_fn)\n",
    "\n",
    "# Création de l'optimiseur\n",
    "initial_lr = 0.02\n",
    "adam = Adam(model.get_parameters(), lr=initial_lr)\n",
    "\n",
    "# Nombre total de points de données à générer\n",
    "nb_data_points = int(5*1e4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd85457",
   "metadata": {},
   "source": [
    "### Initialisation du Dataset et du Scheduler\n",
    "\n",
    "Configuration des paramètres pour la génération des données (plage, taille de batch) et création de l'instance `Dataset`. Création du planificateur de taux d'apprentissage (`LR_Scheduler`) en le liant à l'optimiseur `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c47ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 703 training batches, 78 evaluation batches.\n"
     ]
    }
   ],
   "source": [
    "# Paramètres du dataset et de l'entraînement\n",
    "min_max_value = 10 # Plage des données x: [-10, 10]\n",
    "batch_size = 64\n",
    "nb_epochs = 10\n",
    "eval_ratio = 0.1\n",
    "\n",
    "# Création du dataset\n",
    "dataset = Dataset(min_max_value, nb_data=nb_data_points, eval_ratio=eval_ratio, batch_size=batch_size)\n",
    "\n",
    "# Configuration et création du scheduler\n",
    "final_lr = 1e-7\n",
    "total_training_steps = dataset.train_len() * nb_epochs\n",
    "scheduler = LR_Scheduler(adam, start_value=initial_lr, end_value=final_lr, nb_steps=total_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37357a79",
   "metadata": {},
   "source": [
    "### Boucle d'Entraînement Principale\n",
    "\n",
    "Itère sur les époques et les batches de données d'entraînement.\n",
    "Pour chaque batch :\n",
    "1. **Normalisation**: Met à l'échelle les données d'entrée `x` (ici, division par `min_max_value`). C'est souvent utile pour la stabilité de l'entraînement.\n",
    "2. **Forward Pass**: Calcule les prédictions et la perte.\n",
    "3. **Backward Pass**: Calcule les gradients.\n",
    "4. **Optimizer Step**: Met à jour les paramètres du modèle.\n",
    "5. **Scheduler Step**: Met à jour le taux d'apprentissage.\n",
    "6. **Logging**: Affiche périodiquement la perte.\n",
    "\n",
    "Après chaque époque, une évaluation est effectuée sur l'ensemble de données d'évaluation pour surveiller la performance du modèle sur des données non vues pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd1b9762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Epoch 1/10 ===\n",
      "Epoch 1, Step 200: Avg Loss = 4.745e-01, LR = 1.786e-02\n",
      "Epoch 1, Step 200: Avg Loss = 4.745e-01, LR = 1.786e-02\n",
      "Epoch 1, Step 400: Avg Loss = 4.758e-01, LR = 1.594e-02\n",
      "Epoch 1, Step 400: Avg Loss = 4.758e-01, LR = 1.594e-02\n",
      "Epoch 1, Step 600: Avg Loss = 4.664e-01, LR = 1.422e-02\n",
      "Epoch 1, Step 600: Avg Loss = 4.664e-01, LR = 1.422e-02\n",
      "--- Evaluation Loss: 0.47498 ---\n",
      "=== Starting Epoch 2/10 ===\n",
      "--- Evaluation Loss: 0.47498 ---\n",
      "=== Starting Epoch 2/10 ===\n",
      "Epoch 2, Step 800: Avg Loss = 4.697e-01, LR = 1.269e-02\n",
      "Epoch 2, Step 800: Avg Loss = 4.697e-01, LR = 1.269e-02\n",
      "Epoch 2, Step 1000: Avg Loss = 4.667e-01, LR = 1.133e-02\n",
      "Epoch 2, Step 1000: Avg Loss = 4.667e-01, LR = 1.133e-02\n",
      "Epoch 2, Step 1200: Avg Loss = 4.681e-01, LR = 1.011e-02\n",
      "Epoch 2, Step 1200: Avg Loss = 4.681e-01, LR = 1.011e-02\n",
      "Epoch 2, Step 1400: Avg Loss = 4.615e-01, LR = 9.022e-03\n",
      "--- Evaluation Loss: 0.46723 ---\n",
      "=== Starting Epoch 3/10 ===\n",
      "Epoch 2, Step 1400: Avg Loss = 4.615e-01, LR = 9.022e-03\n",
      "--- Evaluation Loss: 0.46723 ---\n",
      "=== Starting Epoch 3/10 ===\n",
      "Epoch 3, Step 1600: Avg Loss = 4.639e-01, LR = 8.052e-03\n",
      "Epoch 3, Step 1600: Avg Loss = 4.639e-01, LR = 8.052e-03\n",
      "Epoch 3, Step 1800: Avg Loss = 4.657e-01, LR = 7.186e-03\n",
      "Epoch 3, Step 1800: Avg Loss = 4.657e-01, LR = 7.186e-03\n",
      "Epoch 3, Step 2000: Avg Loss = 4.598e-01, LR = 6.413e-03\n",
      "Epoch 3, Step 2000: Avg Loss = 4.598e-01, LR = 6.413e-03\n",
      "--- Evaluation Loss: 0.46584 ---\n",
      "=== Starting Epoch 4/10 ===\n",
      "Epoch 4, Step 2200: Avg Loss = 4.659e-01, LR = 5.723e-03\n",
      "--- Evaluation Loss: 0.46584 ---\n",
      "=== Starting Epoch 4/10 ===\n",
      "Epoch 4, Step 2200: Avg Loss = 4.659e-01, LR = 5.723e-03\n",
      "Epoch 4, Step 2400: Avg Loss = 4.657e-01, LR = 5.108e-03\n",
      "Epoch 4, Step 2400: Avg Loss = 4.657e-01, LR = 5.108e-03\n",
      "Epoch 4, Step 2600: Avg Loss = 4.628e-01, LR = 4.558e-03\n",
      "Epoch 4, Step 2600: Avg Loss = 4.628e-01, LR = 4.558e-03\n",
      "Epoch 4, Step 2800: Avg Loss = 4.563e-01, LR = 4.068e-03\n",
      "--- Evaluation Loss: 0.45109 ---\n",
      "=== Starting Epoch 5/10 ===\n",
      "Epoch 4, Step 2800: Avg Loss = 4.563e-01, LR = 4.068e-03\n",
      "--- Evaluation Loss: 0.45109 ---\n",
      "=== Starting Epoch 5/10 ===\n",
      "Epoch 5, Step 3000: Avg Loss = 3.659e-01, LR = 3.630e-03\n",
      "Epoch 5, Step 3000: Avg Loss = 3.659e-01, LR = 3.630e-03\n",
      "Epoch 5, Step 3200: Avg Loss = 7.098e-02, LR = 3.240e-03\n",
      "Epoch 5, Step 3200: Avg Loss = 7.098e-02, LR = 3.240e-03\n",
      "Epoch 5, Step 3400: Avg Loss = 6.207e-03, LR = 2.891e-03\n",
      "Epoch 5, Step 3400: Avg Loss = 6.207e-03, LR = 2.891e-03\n",
      "--- Evaluation Loss: 0.00137 ---\n",
      "=== Starting Epoch 6/10 ===\n",
      "Epoch 6, Step 3600: Avg Loss = 1.378e-03, LR = 2.580e-03\n",
      "--- Evaluation Loss: 0.00137 ---\n",
      "=== Starting Epoch 6/10 ===\n",
      "Epoch 6, Step 3600: Avg Loss = 1.378e-03, LR = 2.580e-03\n",
      "Epoch 6, Step 3800: Avg Loss = 6.833e-04, LR = 2.303e-03\n",
      "Epoch 6, Step 3800: Avg Loss = 6.833e-04, LR = 2.303e-03\n",
      "Epoch 6, Step 4000: Avg Loss = 4.203e-04, LR = 2.055e-03\n",
      "Epoch 6, Step 4000: Avg Loss = 4.203e-04, LR = 2.055e-03\n",
      "Epoch 6, Step 4200: Avg Loss = 3.201e-04, LR = 1.834e-03\n",
      "--- Evaluation Loss: 0.00032 ---\n",
      "=== Starting Epoch 7/10 ===\n",
      "Epoch 6, Step 4200: Avg Loss = 3.201e-04, LR = 1.834e-03\n",
      "--- Evaluation Loss: 0.00032 ---\n",
      "=== Starting Epoch 7/10 ===\n",
      "Epoch 7, Step 4400: Avg Loss = 2.676e-04, LR = 1.637e-03\n",
      "Epoch 7, Step 4400: Avg Loss = 2.676e-04, LR = 1.637e-03\n",
      "Epoch 7, Step 4600: Avg Loss = 2.117e-04, LR = 1.461e-03\n",
      "Epoch 7, Step 4600: Avg Loss = 2.117e-04, LR = 1.461e-03\n",
      "Epoch 7, Step 4800: Avg Loss = 1.815e-04, LR = 1.304e-03\n",
      "Epoch 7, Step 4800: Avg Loss = 1.815e-04, LR = 1.304e-03\n",
      "--- Evaluation Loss: 0.00016 ---\n",
      "=== Starting Epoch 8/10 ===\n",
      "Epoch 8, Step 5000: Avg Loss = 1.489e-04, LR = 1.163e-03\n",
      "--- Evaluation Loss: 0.00016 ---\n",
      "=== Starting Epoch 8/10 ===\n",
      "Epoch 8, Step 5000: Avg Loss = 1.489e-04, LR = 1.163e-03\n",
      "Epoch 8, Step 5200: Avg Loss = 1.553e-04, LR = 1.038e-03\n",
      "Epoch 8, Step 5200: Avg Loss = 1.553e-04, LR = 1.038e-03\n",
      "Epoch 8, Step 5400: Avg Loss = 1.125e-04, LR = 9.266e-04\n",
      "Epoch 8, Step 5400: Avg Loss = 1.125e-04, LR = 9.266e-04\n",
      "Epoch 8, Step 5600: Avg Loss = 1.092e-04, LR = 8.269e-04\n",
      "--- Evaluation Loss: 0.00010 ---\n",
      "=== Starting Epoch 9/10 ===\n",
      "Epoch 8, Step 5600: Avg Loss = 1.092e-04, LR = 8.269e-04\n",
      "--- Evaluation Loss: 0.00010 ---\n",
      "=== Starting Epoch 9/10 ===\n",
      "Epoch 9, Step 5800: Avg Loss = 8.863e-05, LR = 7.380e-04\n",
      "Epoch 9, Step 5800: Avg Loss = 8.863e-05, LR = 7.380e-04\n",
      "Epoch 9, Step 6000: Avg Loss = 8.307e-05, LR = 6.586e-04\n",
      "Epoch 9, Step 6000: Avg Loss = 8.307e-05, LR = 6.586e-04\n",
      "Epoch 9, Step 6200: Avg Loss = 8.559e-05, LR = 5.878e-04\n",
      "Epoch 9, Step 6200: Avg Loss = 8.559e-05, LR = 5.878e-04\n",
      "--- Evaluation Loss: 0.00006 ---\n",
      "=== Starting Epoch 10/10 ===\n",
      "Epoch 10, Step 6400: Avg Loss = 6.679e-05, LR = 5.245e-04\n",
      "--- Evaluation Loss: 0.00006 ---\n",
      "=== Starting Epoch 10/10 ===\n",
      "Epoch 10, Step 6400: Avg Loss = 6.679e-05, LR = 5.245e-04\n",
      "Epoch 10, Step 6600: Avg Loss = 6.232e-05, LR = 4.681e-04\n",
      "Epoch 10, Step 6600: Avg Loss = 6.232e-05, LR = 4.681e-04\n",
      "Epoch 10, Step 6800: Avg Loss = 6.383e-05, LR = 4.178e-04\n",
      "Epoch 10, Step 6800: Avg Loss = 6.383e-05, LR = 4.178e-04\n",
      "Epoch 10, Step 7000: Avg Loss = 6.175e-05, LR = 3.728e-04\n",
      "--- Evaluation Loss: 0.00005 ---\n",
      "Epoch 10, Step 7000: Avg Loss = 6.175e-05, LR = 3.728e-04\n",
      "--- Evaluation Loss: 0.00005 ---\n"
     ]
    }
   ],
   "source": [
    "logging_loss_sum = 0\n",
    "logging_step_interval = 200 # Afficher la perte tous les 200 pas\n",
    "global_step = 0\n",
    "\n",
    "def normalize(x, max_val):\n",
    "    \"\"\" Normalise les données d'entrée. \"\"\"\n",
    "    # Simple mise à l'échelle dans ce cas\n",
    "    return x / max_val\n",
    "\n",
    "def evaluate(model, dataset, max_val):\n",
    "    \"\"\" Évalue le modèle sur l'ensemble d'évaluation. \"\"\"\n",
    "    eval_log_loss = 0\n",
    "    nb_eval_batches = dataset.eval_len()\n",
    "    if nb_eval_batches == 0:\n",
    "        print(\"No evaluation data.\")\n",
    "        return\n",
    "        \n",
    "    # Itérer sur les batches d'évaluation\n",
    "    for x_eval, y_eval in zip(*dataset.get_eval_data()):\n",
    "        x_eval_norm = normalize(x_eval, max_val)\n",
    "        # Obtenir la perte sans calculer les gradients\n",
    "        _, loss = model(x_eval_norm, y_eval) \n",
    "        eval_log_loss += loss.item() # Utiliser .item() pour obtenir la valeur scalaire\n",
    "    # Afficher la perte moyenne sur l'ensemble d'évaluation\n",
    "    print(f\"--- Evaluation Loss: {eval_log_loss / nb_eval_batches:.5f} ---\")\n",
    "    \n",
    "# Boucle sur les époques\n",
    "for epoch in range(nb_epochs):\n",
    "    print(f\"=== Starting Epoch {epoch+1}/{nb_epochs} ===\")\n",
    "    # Boucle sur les batches d'entraînement\n",
    "    for idx, (x_batch, y_batch) in enumerate(zip(*dataset.get_train_data())):\n",
    "        # 1. Normaliser l'entrée\n",
    "        x_batch_norm = normalize(x_batch, min_max_value)\n",
    "        \n",
    "        # 2. Forward pass: obtenir prédictions et perte\n",
    "        _, loss = model(x_batch_norm, y_batch)\n",
    "        \n",
    "        # 3. Backward pass: calculer les gradients\n",
    "        model.backward()\n",
    "        \n",
    "        # 4. Mise à jour des paramètres par l'optimiseur\n",
    "        adam.step()\n",
    "        \n",
    "        # 5. Mise à jour du taux d'apprentissage\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 6. Logging\n",
    "        logging_loss_sum += loss.item()\n",
    "        global_step += 1\n",
    "        if global_step % logging_step_interval == 0:\n",
    "            avg_loss = logging_loss_sum / logging_step_interval\n",
    "            print(f\"Epoch {epoch+1}, Step {global_step}: Avg Loss = {avg_loss:.3e}, LR = {adam.lr:.3e}\")\n",
    "            logging_loss_sum = 0 # Réinitialiser la somme pour la prochaine période\n",
    "            \n",
    "    # Évaluation à la fin de chaque époque\n",
    "    evaluate(model, dataset, min_max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233373c",
   "metadata": {},
   "source": [
    "### Visualisation des Résultats\n",
    "\n",
    "Génère un ensemble de points `x` sur la plage d'intérêt, les normalise, puis utilise le modèle entraîné pour prédire les valeurs `y` correspondantes. Enfin, trace les prédictions du modèle par rapport à la vraie fonction sinus pour visualiser la qualité de l'approximation apprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c96ccc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Générer des points x pour le tracé\n",
    "x_plot = np.linspace(-min_max_value, min_max_value, 1000)\n",
    "x_plot_reshaped = x_plot.reshape(-1, 1) # Mettre en forme pour le modèle (N, 1)\n",
    "\n",
    "# Normaliser les points x\n",
    "x_plot_norm = normalize(x_plot_reshaped, min_max_value)\n",
    "\n",
    "# Obtenir les prédictions du modèle (sans calculer la perte ni les gradients)\n",
    "y_model_pred, _ = model(x_plot_norm)\n",
    "\n",
    "# Tracer les résultats\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_plot, y_model_pred, label=\"Prédiction Modèle\", color='red')\n",
    "plt.plot(x_plot, np.sin(x_plot), '.', label=\"Vraie Fonction sin(x)\", markersize=2, color='blue', alpha=0.6)\n",
    "plt.title(\"Approximation de sin(x) par le Réseau de Neurones\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(-min_max_value, min_max_value)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7907d",
   "metadata": {},
   "source": [
    "### Conclusion et Discussion\n",
    "\n",
    "Ce notebook a présenté une implémentation d'un réseau de neurones simple \"from scratch\" en utilisant NumPy. Nous avons couvert les concepts fondamentaux suivants :\n",
    "\n",
    "1.  **Structure Modulaire**: L'utilisation des classes `Module`, `Layer`, et `Linear` permet de construire des réseaux de manière flexible et organisée, similaire aux frameworks modernes comme PyTorch ou TensorFlow.\n",
    "2.  **Graphe de Calcul Dynamique (implicite)**: Bien que non explicite comme dans PyTorch, l'enregistrement des fonctions (`Add`, `Mult`, activations) lors du `forward pass` et leur utilisation lors du `backward pass` simule un graphe de calcul dynamique. Chaque opération stocke les informations nécessaires (entrées) pour calculer les gradients locaux.\n",
    "3.  **Autodifférentiation (Backward Propagation)**: La classe `Function` et ses implémentations spécifiques (`Add`, `Mult`, `ReLU`, `Tanh`, `MSELoss`, etc.) sont au cœur de la rétropropagation. Chaque fonction implémente sa propre règle de dérivation (`backward`), permettant de calculer les gradients par rapport à ses entrées en utilisant le gradient reçu de l'opération suivante (règle de chaîne).\n",
    "4.  **Optimisation**: L'implémentation de l'optimiseur `Adam` montre comment les gradients calculés lors de la rétropropagation sont utilisés pour mettre à jour les paramètres apprenables (`Parameter`) du réseau.\n",
    "5.  **Entraînement**: La boucle d'entraînement standard (forward, backward, optimizer step) a été mise en œuvre, incluant la gestion des données par batch, la normalisation, et l'ajustement du taux d'apprentissage (`LR_Scheduler`).\n",
    "\n",
    "**Discussion Théorique vs Implémentation**\n",
    "\n",
    "- La section théorique a détaillé les équations mathématiques de la forward et backward propagation. Le code implémente directement ces équations :\n",
    "    - `Linear.forward` calcule $z^{(n)} = W^{(n)} y^{(n-1)} + b^{(n)}$ en utilisant `Mult` et `Add`.\n",
    "    - `Layer.forward` applique ensuite l'activation $y^{(n)} = \\sigma^{(n)}(z^{(n)})$.\n",
    "    - `MSELoss.forward` calcule $\\mathcal{L}$.\n",
    "    - `MSELoss.backward` calcule $\\frac{\\partial \\mathcal{L}}{\\partial y^{(L)}}$.\n",
    "    - `Activation.backward` calcule $\\delta = \\frac{\\partial \\mathcal{L}}{\\partial y} \\odot \\sigma'(z)$.\n",
    "    - `Linear.backward` utilise les `backward` de `Add` et `Mult` pour calculer $\\frac{\\partial \\mathcal{L}}{\\partial W^{(n)}}$, $\\frac{\\partial \\mathcal{L}}{\\partial b^{(n)}}$ et $\\frac{\\partial \\mathcal{L}}{\\partial y^{(n-1)}}$. La division par $N$ (taille du batch) pour les gradients des poids et biais est effectuée dans `Mult.backward` et `Add.backward` respectivement.\n",
    "- La gestion des dimensions (batch, features) est cruciale et doit être cohérente entre les opérations matricielles (`@`) et les opérations élément par élément (`+`, `*`, activations).\n",
    "- L'initialisation des poids et l'utilisation d'un scheduler de taux d'apprentissage sont des aspects pratiques importants pour la convergence de l'entraînement.\n",
    "\n",
    "**Limitations et Pistes d'Amélioration**\n",
    "\n",
    "- **Efficacité**: L'implémentation NumPy est beaucoup moins efficace que les frameworks optimisés qui utilisent des backends C++/CUDA.\n",
    "\n",
    "- **Hyperparamètres**: La taille du batch, le taux d'apprentissage, le nombre d'époques, et la structure du réseau (nombre de couches et neurones) sont des hyperparamètres cruciaux. Leur choix a un impact significatif sur la convergence et la performance du modèle. Parfois un simple changement du nombre de neurones dans une couche cachée peut faire la différence entre un modèle qui converge et un modèle qui ne converge pas.\n",
    "- **Fonctions d'activation**: D'autres fonctions d'activation (ReLU, Leaky ReLU, etc.) pourraient être ajoutées pour explorer leur impact sur l'entraînement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
